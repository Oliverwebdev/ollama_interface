ğŸš€ **Aktualisierung der README.md**  
Wir bringen die README auf den neuesten Stand und fÃ¼gen alle **neuen Features & Verbesserungen** hinzu:  
- **Neues Styling (OpenAI-Look)**
- **Responsive Design**
- **AnhÃ¤nge (Bilder, PDFs, Text)**
- **Fehlerbehebung & Debugging-Tipps**

---

### **ğŸ“Œ Neue README.md**
```md
# EchoCore - KI Chat Interface

Dieses Projekt stellt eine moderne, interaktive Chat-Anwendung bereit, um Ã¼ber ein React-Frontend und ein Python/FastAPI-Backend mit einer lokalen Ollama-Instanz zu interagieren.  
Es nutzt **modernes Styling (OpenAI-Look)**, **mobile Optimierung** und erlaubt den Upload von **Bildern, PDFs & Textdateien** zur Analyse durch das KI-Modell.

## ğŸ“Œ Inhaltsverzeichnis
* [Ãœberblick](#Ã¼berblick)
* [Technologie-Stack](#technologie-stack)
* [Projektstruktur](#projektstruktur)
* [Voraussetzungen](#voraussetzungen)
* [Installation & Start](#installation--start)
* [Nutzung](#nutzung)
* [Features](#features)
* [Konfiguration](#konfiguration)
* [Fehlersuche](#fehlersuche)
* [ZukÃ¼nftige Erweiterungen](#zukÃ¼nftige-erweiterungen)
* [Lizenz](#lizenz)

---

## ğŸš€ **Ãœberblick**
- **Frontend (React + Vite)** â†’ Moderner Chat mit OpenAI-Optik  
- **Backend (FastAPI)** â†’ Sendet Anfragen an Ollama (lokale KI)  
- **Ollama** â†’ FÃ¼hrt KI-Modelle aus (`deepseek-r1:7b` oder andere)  
- **Docker & Docker Compose** â†’ Container-Management  

### ğŸ”— **Datenfluss**
```txt
[Browser / Frontend] â†’ [FastAPI-Backend in Docker] â†’ [Ollama auf dem Host-System]
```

---

## âš¡ **Technologie-Stack**
- **React mit Vite** (Frontend)
- **Python 3.10 mit FastAPI** (Backend)
- **Docker & Docker Compose**
- **Ollama (Lokaler Modellserver)**

---

## ğŸ“‚ **Projektstruktur**
```
echocore/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile           # Backend (Python) Container Setup
â”‚   â”œâ”€â”€ requirements.txt     # Python-AbhÃ¤ngigkeiten
â”‚   â”œâ”€â”€ main.py              # FastAPI-App (mit /ollama Endpoint)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile           # Frontend (Node/Vite) Container Setup
â”‚   â”œâ”€â”€ package.json         # NPM-Skripte & Dependencies
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx          # React-Hauptkomponente (Chat-UI)
â”‚   â”‚   â”œâ”€â”€ App.css          # Komplettes UI-Styling (Desktop + Mobile)
â”‚   â”‚   â””â”€â”€ Spinner.jsx      # Lade-Animation
â”œâ”€â”€ docker-compose.yml       # Orchestrierung fÃ¼r Frontend + Backend
â”œâ”€â”€ README.md                # Dieses Dokument
â””â”€â”€ .gitignore               # Verhindert unnÃ¶tige Uploads
```

---

## ğŸ›  **Voraussetzungen**
- **Docker (empfohlen Version â‰¥ 20.10)**
- **Docker Compose (â‰¥ 1.29)**
- **Ollama lokal installiert (â‰¥ 0.4.5)**
- **Python nur fÃ¼r lokale Backend-Starts notwendig**

---

## ğŸš€ **Installation & Start**
### **1ï¸âƒ£ Docker Compose starten**
```bash
cd echocore
sudo docker compose up --build
```
- **Frontend lÃ¤uft unter:** `http://localhost:5173`
- **Backend API unter:** `http://localhost:8000/docs`
- **Ollama KI lÃ¤uft auf:** `http://localhost:11434`

---

## ğŸ’¬ **Nutzung**
### **ğŸ“¥ Eingabe**
- Der User gibt eine Nachricht ein.
- Falls **AnhÃ¤nge (Bilder, PDFs, TXT) vorhanden sind**, werden sie mitgesendet.

### **ğŸ”„ Verarbeitung**
- **FastAPI empfÃ¤ngt die Anfrage**  
- Falls eine Datei gesendet wurde:
  - **TXT â†’ Direkt in den Prompt eingefÃ¼gt**
  - **PDF â†’ Text extrahiert & angehÃ¤ngt**
  - **Bilder â†’ OCR-Texterkennung mit Tesseract**
  - **Andere Formate â†’ In Base64 gewandelt**

### **ğŸ“¤ Antwort**
- Ollama verarbeitet die Eingabe & sendet das JSON zurÃ¼ck.
- **Frontend zeigt die Antwort mit OpenAI-Optik im Chat an.**
- Falls `<think>`-Tags erkannt werden, erscheinen diese als **Denkblasen**.

---

## ğŸŒŸ **Features**
âœ… **ğŸ”¹ Modernes Chat-Design (OpenAI-Stil)**  
âœ… **ğŸ“± 100% Responsive (Mobile & Desktop)**  
âœ… **ğŸ“‚ AnhÃ¤nge: PDFs, Bilder, Textdateien analysieren**  
âœ… **ğŸ§  `<think>`-Tags als Denkblasen anzeigen**  
âœ… **ğŸ¨ Light & Dark Mode-UnterstÃ¼tzung (optional)**  
âœ… **âš¡ Schnelle Performance dank Vite & FastAPI**

---

## âš™ï¸ **Konfiguration**
### **ğŸ”§ Frontend (`App.jsx`)**
- **Chat-UI & Denkblasen-Logik**
- **Request an `http://localhost:8000/ollama`**
- **Styling in `App.css` fÃ¼r OpenAI-Optik**

### **ğŸ”§ Backend (`main.py`)**
- **Ollama URL:** `http://host.docker.internal:11434/api/generate`
- **Modelleinstellungen:** Standard `"deepseek-r1:7b"`  
  â†’ Kann fÃ¼r andere Modelle geÃ¤ndert werden.

### **ğŸ”§ Docker Compose**
- **Definiert `frontend` & `backend`**
- `extra_hosts: "host.docker.internal:host-gateway"` fÃ¼r Netzwerkzugriff

---

## ğŸ” **Fehlersuche**
### ğŸ›  **Backend gibt 500-Fehler**
```bash
docker compose logs backend
```
- **LÃ¶sung:** PrÃ¼fe, ob Ollama lÃ¤uft (`ollama serve`).

### ğŸ›  **Frontend zeigt keine Nachrichten**
- **LÃ¶sung:** `F12` â†’ **Console Ã¶ffnen** â†’ Fehler in `messages` Ã¼berprÃ¼fen.

### ğŸ›  **AnhÃ¤nge werden nicht verarbeitet**
- **LÃ¶sung:** Backend-Logs prÃ¼fen (`docker compose logs backend`).
- **Falls OCR nicht funktioniert:** `tesseract-ocr` nachinstallieren.

---

## ğŸš€ **ZukÃ¼nftige Erweiterungen**
âœ… **Live-Streaming von Antworten (`stream: true`)**  
âœ… **Weitere Datei-Formate wie MP3-Transkription**  
âœ… **Offline-Modus fÃ¼r lokale Nutzung**  
âœ… **Modell-Auswahl direkt im Chat**  

---

## ğŸ“œ **Lizenz**
Dieses Projekt ist Open Source. Nutze es, verbessere es & entwickle es weiter! ğŸš€  
Falls du Features beisteuern willst â€“ Pull Requests sind willkommen! ğŸ˜  
```

---

## **ğŸš€ Was ist neu in dieser README?**
âœ… **Aktueller Name (`EchoCore` statt `Ollama Chat Interface`)**  
âœ… **Neue Features (`think`-Tags, Responsive Design, OpenAI-Look)**  
âœ… **Optimierte Projektstruktur & Nutzung von AnhÃ¤ngen**  
âœ… **Fehlersuche-Abschnitt fÃ¼r Debugging-Tipps**  
âœ… **Zukunftsplanung fÃ¼r Erweiterungen**  

---
